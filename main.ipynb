{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('https://theathletic.com/team/liverpool/?rss=1', 'entry', 'id', 'title', 'link', 'href', 'Liverpool FC', 'The Athletic', 'published')\n",
      "10 ('http://www.nytimes.com/services/xml/rss/nyt/Opinion.xml', 'item', 'link', 'title', 'media:content', 'url', 'Self Dev', 'New York Times', 'pubDate')\n",
      "20 ('http://www.nytimes.com/services/xml/rss/nyt/HomePage.xml', 'item', 'link', 'title', 'media:content', 'url', 'Global News', 'New York Times', 'pubDate')\n",
      "No new items to add.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dateutil.parser import parse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def fetch_feed(url, main_tag, link_tag, title_tag, image_tag, image_attr, category, website, date_tag):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml-xml')\n",
    "    entries = []\n",
    "\n",
    "    for entry in soup.find_all(main_tag):\n",
    "        title = entry.find(title_tag).text\n",
    "        link = entry.find(link_tag).text\n",
    "        date = entry.find(date_tag).text\n",
    "        published = normalize_datetime_to_django_format(date)\n",
    "\n",
    "        entries.append([title, link, category, website, published])\n",
    "\n",
    "    return entries\n",
    "\n",
    "def check_and_store_items(items, csv_filename):\n",
    "    # Check if CSV file exists\n",
    "    if os.path.exists(csv_filename):\n",
    "        # Load existing data\n",
    "        df_existing = pd.read_csv(csv_filename)\n",
    "        # Create a set of existing links for fast lookup\n",
    "        existing_links = set(df_existing['link'])\n",
    "    else:\n",
    "        # Initialize an empty DataFrame if the file doesn't exist\n",
    "        df_existing = pd.DataFrame(columns=['title', 'link', 'category', 'website', 'published'])\n",
    "        existing_links = set()\n",
    "\n",
    "    # Filter out items that already exist in the CSV\n",
    "    new_items = [item for item in items if item[1] not in existing_links]\n",
    "\n",
    "    if new_items:\n",
    "        # Convert the new items to a DataFrame\n",
    "        df_new = pd.DataFrame(new_items, columns=['title', 'link', 'category', 'website', 'published'])\n",
    "        # Append new data to existing data\n",
    "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        # Save the combined data to the CSV file\n",
    "        df_combined.to_csv(csv_filename, index=False)\n",
    "        print(f\"{len(new_items)} new items added to the CSV file.\")\n",
    "    else:\n",
    "        print(\"No new items to add.\")\n",
    "\n",
    "rss_feed_details = [\n",
    "    ('https://theathletic.com/team/liverpool/?rss=1', 'entry', 'id', 'title', 'link', 'href', 'Liverpool FC','The Athletic','published'),\n",
    "    ('http://www.thisisanfield.com/feed/', 'item', 'link', 'title', 'enclosure', 'url', 'Liverpool FC','This is Anfield', 'pubDate'),\n",
    "    ('http://www.theguardian.com/football/rss', 'item', 'link', 'title', 'media:content', 'url', 'Football', 'The Guardian', 'pubDate'),\n",
    "    ('https://theathletic.com/premier-league/?rss', 'entry', 'id', 'title', 'link', 'href', 'Football','The Athletic', 'published'),\n",
    "    ('https://theathletic.com/soccer/?rss', 'entry', 'id', 'title', 'link', 'href', 'Football','The Athletic','published'),\n",
    "    ('https://theathletic.com/champions-league/?rss', 'entry', 'id', 'title', 'link', 'href', 'Football','The Athletic', 'published'),\n",
    "    ('https://www.autosport.com/rss/feed/f1', 'item', 'link', 'title', 'enclosure', 'url', 'Formula 1', 'Autosport', 'pubDate'),\n",
    "    ('https://the-race.com/category/formula-1/feed/', 'item', 'link', 'title','media:content', 'url', 'Formula 1', 'The Race', 'pubDate'),\n",
    "    ('https://aeon.co/feed.rss', 'item', 'link', 'title', None, None, 'Self Dev', \"Aeon\", 'pubDate'),\n",
    "    ('https://psyche.co/feed', 'item', 'link', 'title', None,None, 'Self Dev', \"Psyche\", 'pubDate'),\n",
    "    ('http://www.nytimes.com/services/xml/rss/nyt/Opinion.xml', 'item', 'link', 'title', 'media:content', 'url', 'Self Dev', \"New York Times\", 'pubDate'),\n",
    "    ('http://www.nytimes.com/services/xml/rss/nyt/Magazine.xml', 'item', 'link', 'title', 'media:content', 'url', 'Self Dev', \"New York Times\", 'pubDate'),\n",
    "    ('http://www.nytimes.com/services/xml/rss/nyt/Science.xml', 'item', 'link', 'title', 'media:content', 'url', 'Science & Technology', \"New York Times\", 'pubDate'),\n",
    "    ('https://www.popsci.com/rss', 'item', 'link', 'title', 'image', 'url', 'Science & Technology', \"Popular Science\", 'pubDate'),\n",
    "    ('http://www.smithsonianmag.com/rss/innovation/', 'item', 'link', 'title', 'enclosure', 'url', 'Science & Technology', \"Smithsonian\", 'pubDate'),\n",
    "    ('http://www.smithsonianmag.com/rss/latest_articles/', 'item', 'link', 'title', 'enclosure', 'url', 'Science & Technology', \"Smithsonian\", 'pubDate'),\n",
    "    ('http://www.nytimes.com/services/xml/rss/nyt/Travel.xml', 'item', 'link', 'title', 'media:content', 'url', 'Travel', \"New York Times\", 'pubDate'),\n",
    "    ('http://www.nytimes.com/services/xml/rss/nyt/Style.xml', 'item', 'link', 'title', 'media:content', 'url', 'Self Dev', \"New York Times\", 'pubDate'),\n",
    "    ('http://www.nytimes.com/services/xml/rss/nyt/Technology.xml', 'item', 'link', 'title', 'media:content', 'url', 'Science & Technology', \"New York Times\", 'pubDate'),\n",
    "    ('http://www.nytimes.com/services/xml/rss/nyt/Business.xml', 'item', 'link', 'title', 'media:content', 'url', 'Global News', \"New York Times\", 'pubDate'),\n",
    "     ('http://www.nytimes.com/services/xml/rss/nyt/HomePage.xml', 'item', 'link', 'title', 'media:content', 'url', 'Global News', \"New York Times\", 'pubDate'),\n",
    "     ('http://feeds.feedburner.com/dawn-news', 'item', 'link', 'title',  'media:content', 'url', 'Pakistan', \"Dawn\", 'pubDate'),\n",
    "     ('https://feeds.feedburner.com/dawn-news-world', 'item', 'link', 'title',  'media:content', 'url', 'Global News', \"Dawn\", 'pubDate'),\n",
    "    ('https://www.theverge.com/rss/reviews/index.xml', 'entry', 'id', 'title', None, None, 'Science & Technology', 'The Verge', 'published'),\n",
    "    ('https://www.nytimes.com/wirecutter/rss/', 'item', 'link', 'title', 'description', 'src', 'Science & Technology', \"New York Times Wirecutter\", 'pubDate')\n",
    "]\n",
    "\n",
    "def normalize_datetime_to_django_format(dt_str):\n",
    "    dt = parse(dt_str)\n",
    "    return dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "def fetch_feed_with_details(feed_details):\n",
    "    return fetch_feed(*feed_details)\n",
    "\n",
    "items = []\n",
    "i = 0\n",
    "for item in rss_feed_details:\n",
    "    if (i%10 == 0):\n",
    "        print(i, item)\n",
    "    items.extend(fetch_feed_with_details(item))\n",
    "    i+=1\n",
    "\n",
    "check_and_store_items(items, \"rss_file.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(text, openai_api_key):\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "        api_key=openai_api_key, \n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a text processing model. Your task is to extract and return the main body of text from the provided content. Focus on the core text of the article, preserving its paragraph structure. Ignore any extraneous elements such as advertisements, navigation links, or non-article content. Do not exceed the length of 10 sentences.\",\n",
    "        ),\n",
    "        (\"human\", text),\n",
    "    ]\n",
    "    summarized_text = llm.invoke(messages)\n",
    "    return summarized_text\n",
    "\n",
    "# Function to fetch full article content\n",
    "def fetch_full_article(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch article at {url}: Status code {response.status_code}\")\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    article_text = ' '.join(p.text for p in soup.find_all('p'))\n",
    "    if not article_text:\n",
    "        print(f\"No content found at {url}\")\n",
    "    return article_text\n",
    "\n",
    "def fetch_nyt(url):\n",
    "    response = requests.get(\"https://r.jina.ai/\"+url)\n",
    "    return response.text\n",
    "\n",
    "# Function to check if an article's summary already exists and process new items\n",
    "def check_and_store_summarized_items(summarized_items, csv_filename, openai_api_key):\n",
    "    if os.path.exists(csv_filename):\n",
    "        df_existing = pd.read_csv(csv_filename)\n",
    "        existing_links = set(df_existing['link'])\n",
    "    else:\n",
    "        df_existing = pd.DataFrame(columns=['title', 'link', 'summary', 'category', 'website', 'published'])\n",
    "        existing_links = set()\n",
    "\n",
    "    new_summarized_items = []\n",
    "    i = 0\n",
    "    for title, link, category, website, published in summarized_items:\n",
    "        print(\"Summarizing article \", i)\n",
    "        i+=1\n",
    "        if \"nytimes\" not in link:\n",
    "            if link not in existing_links:\n",
    "                article_text = fetch_full_article(link)\n",
    "                if article_text:\n",
    "                    print(f\"Proceeding to summarize text for {title}...\")\n",
    "                    text_object = summarize_text(article_text, openai_api_key)\n",
    "                    sum = text_object.content\n",
    "                    print(sum)\n",
    "                else:\n",
    "                    sum = \"No summary available\"\n",
    "                \n",
    "                summary = \"This is an article by \"+ website + \". \" +sum \n",
    "                new_summarized_items.append([title, link, summary, category, website, published])\n",
    "            else:\n",
    "                print(f\"Article '{title}' already summarized. Skipping...\")\n",
    "        else: \n",
    "            if link not in existing_links:\n",
    "                article_text = fetch_nyt(link)\n",
    "                if article_text:\n",
    "                    print(f\"Proceeding to summarize text for {title}...\")\n",
    "                    text_object = summarize_text(article_text, openai_api_key)\n",
    "                    sum = text_object.content\n",
    "                    print(sum)\n",
    "                else:\n",
    "                    sum = \"No summary available\"\n",
    "                summary = \"This is an article by \"+ website + \". \" +sum \n",
    "                new_summarized_items.append([title, link, summary, category, website, published])\n",
    "\n",
    "\n",
    "    if new_summarized_items:\n",
    "        # Convert the new summarized items to a DataFrame\n",
    "        df_new = pd.DataFrame(new_summarized_items, columns=['title', 'link', 'summary', 'category', 'website', 'published'])\n",
    "        # Append new data to existing data\n",
    "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        # Save the combined data to the CSV file\n",
    "        df_combined.to_csv(csv_filename, index=False)\n",
    "        print(f\"{len(new_summarized_items)} new summaries added to the CSV file.\")\n",
    "    else:\n",
    "        print(\"No new summaries to add.\")\n",
    "    \n",
    "    return new_summarized_items\n",
    "\n",
    "summarized_items =  check_and_store_summarized_items(items, 'structured_documents.csv', openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, page_content, metadata):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "# New list to hold the structured documents\n",
    "structured_documents = []\n",
    "\n",
    "for item in summarized_items:\n",
    "    title, link, summary, category, website, published = item\n",
    "    metadata = {\n",
    "        'source': link,\n",
    "        'title': title,\n",
    "        'description': summary,\n",
    "        'category': category,\n",
    "        'website': website,\n",
    "        'published': published,\n",
    "        'language': 'en-US'  # Assuming the language is English\n",
    "    }\n",
    "    # Create a Document instance\n",
    "    document = Document(page_content=summary, metadata=metadata)\n",
    "    structured_documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "all_splits = text_splitter.split_documents(structured_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=openai_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Martin Zubimendi\")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Liverpool is interested in signing Valencia goalkeeper Giorgi Mamardashvili, with reports indicating active negotiations.\n",
      "- A meeting has been scheduled to discuss Mamardashvili's potential transfer, which may include a loan agreement to ensure he remains a regular starter.\n",
      "- Fábio Carvalho has completed his move from Liverpool to Brentford for a fee that could rise to £27.5 million.\n",
      "- Carvalho spent last season on loan and attracted interest from multiple Premier League clubs, but Brentford met Liverpool’s valuation.\n",
      "- Liverpool is considering an offer from Red Bull Salzburg for 19-year-old midfielder Bobby Clark, who is also sought after by other clubs.\n",
      "- Virgil van Dijk noted there have been \"no changes\" to his contract situation, as players assess the club's direction under the new head coach.\n",
      "- Van Dijk expressed confidence in the club's transfer market decisions and the need for new signings.\n",
      "- Recent developments suggest Liverpool may not pursue a No. 6 this summer, following Martin Zubimendi's decision to stay at his current club.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use 10 sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer = rag_chain.invoke(\"Provide me all of the transfer news on Liverpool FC in bullets\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Aaron Wan-Bissaka has joined West Ham on a seven-year contract after leaving Manchester United.\n",
      "- Manchester United is preparing medicals for Matthijs de Ligt and Noussair Mazraoui from Bayern Munich.\n",
      "- Newcastle is awaiting a response to their third bid for Marc Guéhi.\n",
      "- Conor Gallagher's move to Atlético Madrid has stalled.\n",
      "- Manchester United is pursuing Manuel Ugarte.\n",
      "- Chelsea is looking for a player who can score 45 goals before schools return.\n",
      "- A winger has been loaned to Olympiakos and Urawa Reds after joining from Bodø/Glimt 18 months ago.\n",
      "- A playmaker of North Macedonian heritage made his senior debut for Juventus last season.\n",
      "- Villarreal paid £4m to sign a well-travelled winger for Spain's Euro 2024 squad.\n",
      "- Dominic Solanke has moved to Spurs, and Chelsea signed Pedro Neto from Wolves.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\"Porivde me all of teh transfer news in bullets\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Pakistan's leaders, including President Zardari and PM Shehbaz Sharif, aim to bring economic stability as the country celebrates 77 years of independence.\n",
      "- PM Shehbaz announced efforts to reduce inflation and electricity prices, along with a five-year economic program.\n",
      "- At least 95 people were injured, and a child was killed due to celebratory aerial firing in Karachi on Independence Day.\n",
      "- A total of 95 cases of aerial firing were reported across major hospitals in the city.\n",
      "- Police arrested 14 suspects in Karachi's Central District and five in Korangi District related to the firing incidents.\n",
      "- Tensions between Pakistan and Afghanistan escalated, with clashes reported, resulting in civilian and military casualties.\n",
      "- Gen Munir emphasized the need for accurate information to maintain peace and good relations with Afghanistan.\n",
      "- The army, along with the government, is committed to ensuring the security and welfare of the Baloch people.\n",
      "- A remote-controlled bomb blast occurred at Bugti Stadium in Kharan, but no casualties were reported.\n",
      "- Two explosions in Quetta resulted in two deaths and three injuries.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\"latest pakistani news, in bullets\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime Minister Shehbaz Sharif has pledged to bring economic stability to Pakistan, particularly as the nation celebrates its independence. He emphasized the need for self-introspection to address past mistakes and called for a renewed effort to overcome current crises. During a flag-hoisting ceremony, he highlighted that hard work, honesty, and passion are crucial for the country's development. Sharif announced the launch of a comprehensive five-year economic program aimed at revamping the economy, reducing inflation, and lowering electricity prices. He also expressed a commitment to tackling issues such as electricity theft, which costs the country Rs500 billion annually. The government is working on providing low-cost electricity and discussing the use of locally sourced coal to reduce dependency on expensive imports. Additionally, the prime minister plans to implement reforms to enhance governance and stimulate export-led growth. Overall, his focus is on creating a sustainable economic environment for the nation.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke(\"I'd like to know more about: PM Shehbaz Sharif pledged economic stability and a five-year economic program for Pakistan\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
